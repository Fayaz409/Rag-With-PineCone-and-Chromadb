{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6HyTDgKjyaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5649e3cc-5903-486b-9334-619e5c2b99ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.10.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.26.18)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install openai\n",
        "! pip install chromadb\n",
        "! pip install langchain-google-genai\n",
        "! pip install tiktoken\n",
        "! pip install langchain\n",
        "! pip install pinecone-client\n",
        "! pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.conversation.memory import (\n",
        "    ConversationBufferMemory,\n",
        "    ConversationBufferWindowMemory,\n",
        "    ConversationSummaryBufferMemory\n",
        ")\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "import os\n",
        "import getpass\n"
      ],
      "metadata": {
        "id": "_2G11kLHk_vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY=getpass.getpass('Put Pinecone API Here')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmIp1JsTGscc",
        "outputId": "b160113b-f469-4653-bca1-24af92209baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Put Pinecone API Here··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY=getpass.getpass('Openai Key Here')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8J13aRtImvN",
        "outputId": "e103ae54-0599-4269-b311-fe0e1f6d0a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Openai Key Here··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdf"
      ],
      "metadata": {
        "id": "4whgRmuWJez1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43e3f6d-6012-45fd-b956-9f6d824b7d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘pdf’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader=PyPDFDirectoryLoader('/content/pdf',glob='./*.pdf')"
      ],
      "metadata": {
        "id": "mW-obBZEKQ16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=loader.load()"
      ],
      "metadata": {
        "id": "YtpbLUn8Kk1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks(documents,chunk_size:int=2000,chunk_overlap:int=500):\n",
        "  d_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
        "  return d_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "XQBMVRUdKou8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks=get_chunks(documents)"
      ],
      "metadata": {
        "id": "Jr8FX_lwL_AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY']=OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "V5cbFF54MEv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding=OpenAIEmbeddings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLzKOvL5MGes",
        "outputId": "4c482a35-4c32-426e-ba91-eb92ce3b11cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_ENV=os.environ.get('PINECONE_API_ENV','gcp-starter')"
      ],
      "metadata": {
        "id": "DGL_5t1pUnPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed=embedding.embed_query('Hello World')"
      ],
      "metadata": {
        "id": "U6t1kZZXPRHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz76YvWXPqlC",
        "outputId": "4a1002d3-5f42-4ef3-e5cf-9938591fba58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "pc=Pinecone(\n",
        "    api_key=PINECONE_API_KEY\n",
        ")\n"
      ],
      "metadata": {
        "id": "9edPbQIcbY9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name='pinecone'"
      ],
      "metadata": {
        "id": "IWInWn4Njqp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index=pc.Index(index_name)"
      ],
      "metadata": {
        "id": "1EYl0Ru9xx-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "id": "gUkuysAmstxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid"
      ],
      "metadata": {
        "id": "eOhbGI_vwtmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,chunk in enumerate(chunks):\n",
        "  metadatas=[{\n",
        "      'title':chunk.metadata['source'],\n",
        "      'text':chunk.page_content\n",
        "  }]\n",
        "  unique_id = str(uuid.uuid4())\n",
        "  embed=embedding.embed_documents(chunk.page_content)\n",
        "  index.upsert(vectors=zip(unique_id,embed,metadatas))\n"
      ],
      "metadata": {
        "id": "JX0NZe72hDAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LQd5ONulY8Q",
        "outputId": "f61cca34-3cd1-40a7-8f86-28e97f433e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.00013,\n",
              " 'namespaces': {'': {'vector_count': 13}},\n",
              " 'total_vector_count': 13}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_field='text'"
      ],
      "metadata": {
        "id": "AT1cg3crz9wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"  # the metadata field that contains our text\n",
        "\n",
        "# initialize the vector store object\n",
        "vectorstore = Pinecone(\n",
        "    index, embedding.embed_query, text_field\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG_BYVSe2veT",
        "outputId": "9ea173e0-59d1-4ae0-ed62-d26e181d7481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-west-2\"\n",
        ")"
      ],
      "metadata": {
        "id": "yb4CcOEP-5Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "index_name = \"langchain-retrieval-agent\"\n",
        "existing_indexes = [\n",
        "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
        "]\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in existing_indexes:\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of ada 002\n",
        "        metric='dotproduct',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "time.sleep(1)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN38gG87_A7c",
        "outputId": "8e3a4490-c4c4-44d6-f8fb-c16ed00afba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5sMLpIzEkEl",
        "outputId": "5233c6f7-fa05-4368-d35d-e875a3508e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 9}},\n",
              " 'total_vector_count': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,chunk in enumerate(chunks):\n",
        "  metadatas=[{\n",
        "      'title':chunk.metadata['source'],\n",
        "      'text':chunk.page_content\n",
        "  }]\n",
        "  unique_id = str(uuid.uuid4())\n",
        "  embed=embedding.embed_documents(chunk.page_content)\n",
        "  index.upsert(vectors=zip(unique_id,embed,metadatas))"
      ],
      "metadata": {
        "id": "C8oMliP9_Ybv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "text_field = \"text\"\n",
        "vectordb=Pinecone(\n",
        "    index,\n",
        "    embedding.embed_query,\n",
        "    text_field\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCRJNLw1CGh_",
        "outputId": "61140b41-f465-49bc-ac46-4cb095bb1462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the vector store object\n",
        "query = \"In Which University did Fayaz Ali apply?\"\n",
        "\n",
        "vectordb.similarity_search(\n",
        "    query,  # our search query\n",
        "    k=3  # return 3 most relevant docs\n",
        ")"
      ],
      "metadata": {
        "id": "NbhF5wNQzcRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "uYQdSuFxz2S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_bot=RetrievalQA.from_chain_type(llm=OpenAI(),chain_type='stuff',retriever=retriever,return_source_documents=True)"
      ],
      "metadata": {
        "id": "dBsDQRIe4CDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=qa_bot('In project Fine-tune LLMs and build your application. What is required by me? Give me all the details')"
      ],
      "metadata": {
        "id": "svXy8Wxr5LY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYHiTeMV5pMs",
        "outputId": "38f83044-8c74-4bce-869a-ae179716c201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'In project Fine-tune LLMs and build your application. What is required by me? Give me all the details',\n",
              " 'result': ' In the project Fine-tune LLMs and build your application, you are required to establish a comprehensive framework that streamlines the fine-tuning process of Large Language Models (LLMs) and provides a user-friendly interface for building applications on top of these tailored language models. This includes addressing key issues such as integration complexity, fine-tuning accessibility, and application development barriers. You will also need to curate a diverse dataset specific to the Indian continent and maintain your code on GitHub. Additionally, you must use a given dataset (MongoDB and MySQL) and can choose to host your solution on a cloud platform such as AWS, Azure, or GCP. Finally, you must expose your solution as an API or create a user interface for model testing. ',\n",
              " 'source_documents': [Document(page_content='1   \\n \\n  \\n \\n \\n \\nProject  Title Fine-tune  LLMs  and build  your  application  \\nTechnologies  Large Language Model  \\nDomain  Generative  AI \\nProject  Difficulties  level  Advance  \\n \\nProblem  Statement:  \\nIn the rapidly evolving landscape of natural language processing, there is a growing demand for  \\naccessible and efficient solutions that enable developers and businesses to harness the  \\ncapabilities of Large Language Models (LLMs) for creating bespoke applications. The challenge  \\nlies in the absence of a unified platform that seamlessly integrates leading technologies such as  \\nLangchain, Hugging Face, LLama2, and Mistral 7b, to facilitate the fine -tuning of LLMs for \\ndiverse  use cases.  \\n \\nDevelopers currently face hurdles in adapting state -of-the-art language models to their specific  \\nrequirements,  limiting  the potential  for innovation  and customization  in applications.  This project  \\nseeks to address these challenges by establishing a comprehensive framework that not only  \\nstreamlines the fine -tuning process but also provides a user -friendly interface for building  \\napplications on top of  these tailored language models.  \\n \\nKey issues to address include:  \\n \\n1. Integration  Complexity:  The integration of  Langchain,  Hugging Face,  LLama2,  and Mistral \\n7b poses  challenges  in terms  of compatibility  and interoperability.  The project  needs  to tackle  \\nthe complexity associated with seamlessly incorporating these technologies into a cohesive  \\nplatform.  \\n2. Fine-tuning  Accessibility:  Developers  currently  lack a straightforward  method  for fine-tuning  \\nLLMs to suit specific application needs. The project must devise user -friendly tools and  \\ndocumentation to democratize  the fine -tuning process and make it accessible to a broader  \\naudience.  \\n3. Application Development Barriers: The absence of a unified platform for building  \\napplications  on top of fine-tuned  language  models  hinders  developers  from \\nefficiently.', metadata={'title': '/content/pdf/Fine-tune LLMs and build your application after editing (2).pdf'}),\n",
              "  Document(page_content='1 \\nClick here to enter text.  \\n \\n                                                                    \\nProject Title  Voice Assistant  \\nTechnologies  Deep Learning, NLP  \\nDomain  Home Automation  \\nProject Difficulties level  Advance  \\n \\n \\nProblem Statement:  \\n1. A voice assistant  is a digital assistant that uses voice recognition , language \\nprocessing algorithms, and voice synthesis to listen to specific voice commands \\nand return relevant information or perform specific functions as requested by the \\nuser.  \\n2. It should be capa ble of voice interaction, music playback, making to -do lists, \\nsetting alarms, streaming podcasts, playing audiobooks, and providing weather, \\ntraffic, sports, and other real -time information, such as news.  \\n3. It should be able to train itself based on our live  interaction with it.  \\n4. Home Automation should be an added feature it should have.  \\n5. You have to build the entire model from scratch.  \\n \\n \\nDataset:  \\nYou have to collect your dataset for this project for the Indian continent voice and from \\nanother continent also, a nd based on that, you have to design your solution and create a \\nrepo for the dataset.  \\n \\nProject Evaluation metrics:  \\nCode:  \\n• You are supposed to write a code in a modular fashion  \\n• Safe: It can be used without causing harm.  \\n• Testable: It can be tested at the code level.  \\n• Maintainable: It can be maintained, even as your codebase grows.  \\n• Portable: It works the same in every environment (operating system)  \\n• You have to maintain your code on GitHub.', metadata={'title': '/content/pdf/Voice Assistant.pdf'}),\n",
              "  Document(page_content='3   \\n \\n \\nDataset:  \\nDataset  Repository  for \"Fine -tune LLMs  & Build  Your  Application”  is OpenAssistant/oasst1 . \\nCurate a diverse dataset specific to the Indian continent, addressing linguistic and cultural  \\nnuances.  The dataset  repository,  including  train and validation  sets,  is accessible  at: \\n \\n- Train  Dataset  \\n- Validation  Dataset  \\n \\n \\nProject  Evaluation  metrics:  \\nCode:  \\n• You are supposed  to write  a code  in a modular  fashion  \\n• Safe:  It can be used  without  causing harm.  \\n• Testable:  It can be tested  at the code  level.  \\n• Maintainable:  It can be maintained,  even  as your codebase  grows.  \\n• Portable:  It works  the same  in every  environment  (operating  system)  \\n• You have  to maintain  your code  on GitHub.  \\n• You have  to keep  your GitHub  repo public  so that anyone  can check  your code.  \\n• Proper  readme  file you have  to maintain  for any project  development.  \\n• You should  include  basic  workflow  and execution  of the entire  project  in the readme  \\nfile on  GitHub.  \\n• Follow  the coding  standards:  https:// www.python.org/dev/peps/pep -0008/  \\n \\n \\nDatabase:  \\n• You are supposed  to use a given  dataset  for this project  which  is a MongoDB  \\nand MySQL  database.  \\n• https://astra.dev/ineuron  \\n \\nCloud:  \\n• You can use any cloud  platform  for this entire  solution  hosting like  AWS,  Azure  or \\nGCP  \\nAPI Details  or User  Interface:  \\n• You must  expose  your complete  solution  as an API or try to create  a user \\ninterface  for your model testing.  Anything will  be fine  for us.', metadata={'title': '/content/pdf/Fine-tune LLMs and build your application after editing (2).pdf'}),\n",
              "  Document(page_content='Fayaz Ali  \\n   \\nDate of birth: 21/02/1999  Nationality: Pakistani   \\nHome : Village Ghari Allah Warayio Tunio P.O Bakrani Taluka Bakrani District\\nLarkana, 77150, Larkana, Pakistan \\uf3c5\\nEmail: fayaztunio06@gmail.com  \\uf0e0 Phone: (+92) 03223600975\\uf879\\nOther language(s): \\nEnglish, \\nLISTENING B2 READING B1 WRITING B1 \\nSPOKEN PRODUCTION B2 SPOKEN INTERACTION B2 EDUCATION AND\\nTRAINING  \\n[ 14/09/2020 – 12/06/2024 ]  Bachelor of Mathematics \\nNational University of Science and Technology https://nust.edu.pk/  \\nCity: Islamabad \\nCountry: Pakistan \\nLANGUAGE SKILLS  \\nMother tongue(s): Sindhi , Urdu \\nLevels: A1 and A2: Basic user; B1 and B2: Independent user; C1 and C2: Proﬁcient user\\nPROJECTS  \\n Food Image Classiﬁcation with Transfer Learning  \\n•Utilized a pre-trained ResNet model for transfer learning to classify images within asubset (3 classes) of the Food-101 dataset.\\n•Adapted pre-trained ResNet architecture by modifying the ﬁnal classiﬁcation layer.\\n•Fine-tuned the model for the speciﬁc food classiﬁcation task.\\n•Understanding of image processing, CNN architectures, model training andevaluation.• Demonstrated ability to leverage pre-trained models for eﬃcient training andpotentially improved performance.\\nLink: https://github.com/Fayaz409/Computer-Vision/blob/main/\\nComputer_Vision_Food101.ipynb  \\n MNIST Image Classiﬁer  \\n• Developed an image classiﬁcation model using the Fashion-MNIST dataset from\\ntorchvision.\\n• Implemented image preprocessing techniques (resizing, normalization, etc.).\\n• Designed and trained a convolutional neural network (CNN) architecture for accurate\\nimage classiﬁcation.\\n• Evaluated model performance using appropriate metrics (accuracy, precision, recall,\\netc.).\\nLink: https://github.com/Fayaz409/Computer-Vision/blob/main/Computer Vision Fashion\\nMNIST.ipynb  \\n1 / 2', metadata={'title': '/content/pdf/Fayaz Ali (1).pdf'})]}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PArBhY1Q5q1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}