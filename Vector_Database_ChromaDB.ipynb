{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoMmPjZ82j5U"
      },
      "source": [
        "# Langchain & ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9POQFQUK32g2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c65f1ad-ee7a-45a8-8341-de92a7d98d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: langchain-google-genai\n",
            "Successfully installed langchain-google-genai-0.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet  langchain-google-genai\n",
        "!pip install PyPDF2\n",
        "!pip install langchain\n",
        "!pip install chromadb\n",
        "! pip install OpenAI ## Will Be using this one In this\n",
        "! pip install unstructured\n",
        "!pip install tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQxu0b8G3Cbb"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "import getpass\n",
        "import os\n",
        "import pandas as pd\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "import google.generativeai as genai1\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings as ge\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI as genai\n",
        "from langchain.chains import LLMChain,ConversationChain\n",
        "from langchain.chains.conversation.memory import (\n",
        "    ConversationBufferMemory,\n",
        "    ConversationSummaryBufferMemory,\n",
        "    ConversationKGMemory,\n",
        "    ConversationSummaryMemory\n",
        ")\n",
        "  # Import a PDF library\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMG9RXp52wpK",
        "outputId": "42ca9799-fe5b-461a-f2db-769eab5ddbef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provide your Google API key here¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API key here\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_text(pdf_path):\n",
        "    with open(pdf_path,'rb') as file:\n",
        "        docs=PdfReader(file)\n",
        "        text=''\n",
        "        num_pages=len(docs.pages)\n",
        "        for page_num in range(num_pages):\n",
        "          page=docs.pages[page_num]\n",
        "          text+=page.extract_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "Ut3wvoyAH_A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_splitter(text,chunk_size:int=2000,chunk_overlap:int=500):\n",
        "  t_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
        "  return t_splitter.split_documents(text)"
      ],
      "metadata": {
        "id": "ZM-0H4-XNRrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_file(file_name,text):\n",
        "  with open(file_name,'w') as file:\n",
        "    file.write(text)\n"
      ],
      "metadata": {
        "id": "mGqSCYHETIyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=pdf_to_text('LoRA_Paper.pdf')"
      ],
      "metadata": {
        "id": "M-9x7cyBIpwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_file('Lora.txt',text)"
      ],
      "metadata": {
        "id": "2JqO8pwzTfza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=DirectoryLoader('/content',glob='./*.txt')"
      ],
      "metadata": {
        "id": "ripO3eJmQ5gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documnets=loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZifMvaHRI4z",
        "outputId": "467e7352-dd2f-4d8e-fc24-d50ae70aa85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks=text_splitter(documnets)"
      ],
      "metadata": {
        "id": "tOA4O823Mvr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwC4kORIOQ5-",
        "outputId": "63588a7b-19ae-439a-94c2-67c1d473ca25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='LORA: L OW-RANK ADAPTATION OF LARGE LAN-\\n\\nGUAGE MODELS\\n\\nEdward Hu\\x03Yelong Shen\\x03Phillip Wallis Zeyuan Allen-Zhu\\n\\nYuanzhi Li Shean Wang Lu Wang Weizhu Chen\\n\\nMicrosoft Corporation\\n\\nfedwardhu, yeshe, phwallis, zeyuana,\\n\\nyuanzhil, swang, luw, wzchen g@microsoft.com\\n\\nyuanzhil@andrew.cmu.edu\\n\\n(Version 2)\\n\\nABSTRACT\\n\\nAn important paradigm of natural language processing consists of large-scale pre-\\n\\ntraining on general domain data and adaptation to particular tasks or domains. As\\n\\nwe pre-train larger models, full Ô¨Åne-tuning, which retrains all model parameters,\\n\\nbecomes less feasible. Using GPT-3 175B as an example ‚Äì deploying indepen-\\n\\ndent instances of Ô¨Åne-tuned models, each with 175B parameters, is prohibitively\\n\\nexpensive. We propose Low-RankAdaptation, or LoRA, which freezes the pre-\\n\\ntrained model weights and injects trainable rank decomposition matrices into each\\n\\nlayer of the Transformer architecture, greatly reducing the number of trainable pa-\\n\\nrameters for downstream tasks. Compared to GPT-3 175B Ô¨Åne-tuned with Adam,\\n\\nLoRA can reduce the number of trainable parameters by 10,000 times and the\\n\\nGPU memory requirement by 3 times. LoRA performs on-par or better than Ô¨Åne-\\n\\ntuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\\n\\ning fewer trainable parameters, a higher training throughput, and, unlike adapters,\\n\\nno additional inference latency . We also provide an empirical investigation into\\n\\nrank-deÔ¨Åciency in language model adaptation, which sheds light on the efÔ¨Åcacy of\\n\\nLoRA. We release a package that facilitates the integration of LoRA with PyTorch\\n\\nmodels and provide our implementations and model checkpoints for RoBERTa,\\n\\nDeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .\\n\\n1 I NTRODUCTION\\n\\nPretrained\\n\\nWeights\\n\\nùëä‚àà‚Ñùùëë√óùëë\\n\\nxh\\n\\nùêµ=0\\n\\nùê¥=ùí©(0,ùúé2)\\n\\nùëëùëüPretrained\\n\\nWeights\\n\\nùëä‚àà‚Ñùùëë√óùëë\\n\\nxf(x)\\n\\nùëë\\n\\nFigure 1: Our reparametriza-\\n\\ntion. We only train AandB.Many applications in natural language processing rely on adapt-', metadata={'source': '/content/Lora.txt'})"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding=OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "jwewLm7BPB8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory='db'"
      ],
      "metadata": {
        "id": "OsnYtTmFOyUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb=Chroma.from_documents(documents=chunks,embedding=embedding,persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "6d0idm70PzUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb.persist()\n",
        "vectordb=None"
      ],
      "metadata": {
        "id": "mCidSTw1NDIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb=Chroma(persist_directory=persist_directory,embedding_function=embedding)"
      ],
      "metadata": {
        "id": "OjEq5u5HWr8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=vectordb.as_retriever(search_kwargs={'k':3})"
      ],
      "metadata": {
        "id": "OKffCVWeXPR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs=retriever.get_relevant_documents('Why QLora is better')"
      ],
      "metadata": {
        "id": "jgtaSPJ9XeKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgoY1-54X1ud",
        "outputId": "d82e0018-189d-4e7e-91e5-5f6dad0dd7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain=RetrievalQA.from_chain_type(llm=OpenAI(),chain_type='stuff',retriever=retriever,return_source_documents=True)"
      ],
      "metadata": {
        "id": "7B2MPrQhX3Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=qa_chain('How does QLaRA Work ')"
      ],
      "metadata": {
        "id": "CVRf1xiSd0Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLdqzKzlepNX",
        "outputId": "d099913b-c759-4663-8641-f08af6c65bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'How does QLaRA Work ',\n",
              " 'result': ' LoRA works by reducing the storage and memory requirements for training models by adapting only a subset of weight matrices, rather than the entire model. This allows for more efficient training and lower hardware barriers to entry. Additionally, LoRA allows for easier task-switching and the creation of customized models that can be swapped in and out on the fly. It also provides a 25% speedup during training compared to full fine-tuning.',\n",
              " 'source_documents': [Document(page_content='ferent tasks. We can freeze the shared model and efÔ¨Åciently switch tasks by replacing the\\n\\nmatricesAandBin Figure 1, reducing the storage requirement and task-switching over-\\n\\nhead signiÔ¨Åcantly.\\n\\nLoRA makes training more efÔ¨Åcient and lowers the hardware barrier to entry by up to 3\\n\\ntimes when using adaptive optimizers since we do not need to calculate the gradients or\\n\\nmaintain the optimizer states for most parameters. Instead, we only optimize the injected,\\n\\nmuch smaller low-rank matrices.\\n\\nOur simple linear design allows us to merge the trainable matrices with the frozen weights\\n\\nwhen deployed, introducing no inference latency compared to a fully Ô¨Åne-tuned model, by\\n\\nconstruction.\\n\\nLoRA is orthogonal to many prior methods and can be combined with many of them, such\\n\\nas preÔ¨Åx-tuning. We provide an example in Appendix E.\\n\\nTerminologies and Conventions We make frequent references to the Transformer architecture\\n\\nand use the conventional terminologies for its dimensions. We call the input and output di-\\n\\nmension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\\n\\nquery/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\\n\\ntrained weight matrix and \\x01Wits accumulated gradient update during adaptation. We use rto\\n\\ndenote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\\n\\nBrown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\\n\\noptimization and use a Transformer MLP feedforward dimension dffn= 4\\x02dmodel .\\n\\n2 P ROBLEM STATEMENT\\n\\nWhile our proposal is agnostic to training objective, we focus on language modeling as our motivat-\\n\\ning use case. Below is a brief description of the language modeling problem and, in particular, the\\n\\nmaximization of conditional probabilities given a task-speciÔ¨Åc prompt.\\n\\nSuppose we are given a pre-trained autoregressive language model P\\x08(yjx)parametrized by \\x08.', metadata={'source': '/content/Lora.txt'}),\n",
              "  Document(page_content='ferent tasks. We can freeze the shared model and efÔ¨Åciently switch tasks by replacing the\\n\\nmatricesAandBin Figure 1, reducing the storage requirement and task-switching over-\\n\\nhead signiÔ¨Åcantly.\\n\\nLoRA makes training more efÔ¨Åcient and lowers the hardware barrier to entry by up to 3\\n\\ntimes when using adaptive optimizers since we do not need to calculate the gradients or\\n\\nmaintain the optimizer states for most parameters. Instead, we only optimize the injected,\\n\\nmuch smaller low-rank matrices.\\n\\nOur simple linear design allows us to merge the trainable matrices with the frozen weights\\n\\nwhen deployed, introducing no inference latency compared to a fully Ô¨Åne-tuned model, by\\n\\nconstruction.\\n\\nLoRA is orthogonal to many prior methods and can be combined with many of them, such\\n\\nas preÔ¨Åx-tuning. We provide an example in Appendix E.\\n\\nTerminologies and Conventions We make frequent references to the Transformer architecture\\n\\nand use the conventional terminologies for its dimensions. We call the input and output di-\\n\\nmension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\\n\\nquery/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\\n\\ntrained weight matrix and \\x01Wits accumulated gradient update during adaptation. We use rto\\n\\ndenote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\\n\\nBrown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\\n\\noptimization and use a Transformer MLP feedforward dimension dffn= 4\\x02dmodel .\\n\\n2 P ROBLEM STATEMENT\\n\\nWhile our proposal is agnostic to training objective, we focus on language modeling as our motivat-\\n\\ning use case. Below is a brief description of the language modeling problem and, in particular, the\\n\\nmaximization of conditional probabilities given a task-speciÔ¨Åc prompt.\\n\\nSuppose we are given a pre-trained autoregressive language model P\\x08(yjx)parametrized by \\x08.', metadata={'source': '/content/Lora.txt'}),\n",
              "  Document(page_content='In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\n\\nnumber of trainable parameters. In the Transformer architecture, there are four weight matrices in\\n\\nthe self-attention module ( Wq;Wk;Wv;Wo) and two in the MLP module. We treat Wq(orWk,Wv)\\n\\nas a single matrix of dimension dmodel\\x02dmodel , even though the output dimension is usually sliced\\n\\ninto attention heads. We limit our study to only adapting the attention weights for downstream\\n\\ntasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\\n\\nand parameter-efÔ¨Åciency.We further study the effect on adapting different types of attention weight\\n\\nmatrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\\n\\nlayers, LayerNorm layers, and biases to a future work.\\n\\nPractical BeneÔ¨Åts and Limitations. The most signiÔ¨Åcant beneÔ¨Åt comes from the reduction in\\n\\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\\n\\nusage by up to 2=3ifr\\x1cdmodel as we do not need to store the optimizer states for the frozen\\n\\nparameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\\n\\n350GB. With r= 4and only the query and value projection matrices being adapted, the checkpoint\\n\\nsize is reduced by roughly 10,000 \\x02(from 350GB to 35MB)4. This allows us to train with signiÔ¨Å-\\n\\ncantly fewer GPUs and avoid I/O bottlenecks. Another beneÔ¨Åt is that we can switch between tasks\\n\\nwhile deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\\n\\nparameters. This allows for the creation of many customized models that can be swapped in and out\\n\\non the Ô¨Çy on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup\\n\\nduring training on GPT-3 175B compared to full Ô¨Åne-tuning5as we do not need to calculate the\\n\\ngradient for the vast majority of the parameters.', metadata={'source': '/content/Lora.txt'})]}"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint"
      ],
      "metadata": {
        "id": "gUJvReq3iogG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_res(llm_response):\n",
        "  user_query=llm_response['query']\n",
        "  llm_res=llm_response['result']\n",
        "  print(f\"{user_query}\")\n",
        "  print(f\"\\n{llm_res}\")\n",
        "  print('\\n\\n')\n",
        "  source=[]\n",
        "  for _ in llm_response['source_documents']:\n",
        "    print(_.metadata['source'])"
      ],
      "metadata": {
        "id": "ee6iINmJep3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_res(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYhTI1j6hk56",
        "outputId": "ea64408d-668e-4086-8491-4d9d7ce2a35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How does QLaRA Work \n",
            "\n",
            " LoRA works by reducing the storage and memory requirements for training models by adapting only a subset of weight matrices, rather than the entire model. This allows for more efficient training and lower hardware barriers to entry. Additionally, LoRA allows for easier task-switching and the creation of customized models that can be swapped in and out on the fly. It also provides a 25% speedup during training compared to full fine-tuning.\n",
            "\n",
            "\n",
            "\n",
            "/content/Lora.txt\n",
            "/content/Lora.txt\n",
            "/content/Lora.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lQiu9GVvhpwe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}